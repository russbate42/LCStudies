{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TopoCluster Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simple, stripped-down notebook for training networks. I've removed most of the models that are present in `TopoClusterRegressionRewrite.ipynb`, as well as most of the plots (I find that the multitude of plots makes things a bit cumbersome and hard to navigate -- I'll see if I can change the way they are displayed later on).\n",
    "\n",
    "Here, we just train the so-called `all` model, which uses images from all $6$ calo layers. We train two versions, for charged and neutral pions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML fitting/loading/saving settings\n",
    "loadModel = True # if false, then run trainings directly. otherwise load the file.\n",
    "saveModel = True # if true, save the current model to disk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's choose our training data (and associated strategy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data choice\n",
    "# options are jet, pion, pion_reweighted\n",
    "strat = 'pion_reweighted'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.22/02\n"
     ]
    }
   ],
   "source": [
    "# Import some basic libraries.\n",
    "import sys, os, uuid, glob\n",
    "import numpy as np\n",
    "import pandas as pd # we will use some uproot/pandas interplay here.\n",
    "import uproot as ur\n",
    "import ROOT as rt # used for plotting\n",
    "\n",
    "# Import our resolution utilities\n",
    "path_prefix = os.getcwd() + '/../'\n",
    "if(path_prefix not in sys.path): sys.path.append(path_prefix)\n",
    "from  util import resolution_util as ru\n",
    "from  util import plot_util as pu\n",
    "from  util import ml_util as mu\n",
    "from  util import qol_util as qu # for progress bar\n",
    "\n",
    "rt.gStyle.SetOptStat(0)\n",
    "# use our custom dark style for plots\n",
    "dark_style = qu.PlotStyle('dark')\n",
    "dark_style.SetStyle() # still need to manually adjust legends, paves\n",
    "\n",
    "plotpath = path_prefix + 'regression/Plots/'\n",
    "modelpath = path_prefix + 'regression/Models/'\n",
    "paths = [plotpath, modelpath]\n",
    "for path in paths:\n",
    "    try: os.makedirs(plotpath)\n",
    "    except: pass\n",
    "\n",
    "# metadata\n",
    "layers = [\"EMB1\", \"EMB2\", \"EMB3\", \"TileBar0\", \"TileBar1\", \"TileBar2\"]\n",
    "cell_size_phi = [0.098, 0.0245, 0.0245, 0.1, 0.1, 0.1]\n",
    "cell_size_eta = [0.0031, 0.025, 0.05, 0.1, 0.1, 0.2]\n",
    "len_phi = [4, 16, 16, 4, 4, 4]\n",
    "len_eta = [128, 16, 8, 4, 4, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Get the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let me lay out some definitions, so it's clear as to what the data is.\n",
    "\n",
    "We have a number of different \"strategies\" (the `strat` variable at the top). These correspond to different choices of training, validation and testing datasets.\n",
    "\n",
    "1. `pion`: We train and validate the network using our pion gun data.\n",
    "\n",
    "2. `pion_reweighted`: This is the same as `pion`, except that our training data is reweighted using a jet dataset (via their reco topo-cluster $p_T$ distributions), that corresponds with QCD dijet events.\n",
    "\n",
    "3. `jet`: We train and validate the network using our jet data. This is a facsimile dataset -- we do not know the actual labels of the jet data topo-clusters, so we have assigned labels by matching clusters to truth-level pions in $(\\eta,\\phi)$.\n",
    "\n",
    "The validation performed for these networks is effectively being done on some \"holdout\" dataset from training -- it will by definition have similar kinematics, being drawn from the same set of events. The more interesting test -- how our energy regression performs in tandem with classification on our *unlabeled* jet dataset, will be handled in a separate notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now determine which files we get training and validation data from. Depends on our strategy.\n",
    "if(strat == 'pion' or strat == 'pion_reweighted'):\n",
    "    data_dir = path_prefix + 'data/pion/'\n",
    "    data_filenames = {'pp':data_dir+'piplus.root','pm':data_dir+'piminus.root','p0':data_dir+'pi0.root'}\n",
    "    \n",
    "elif(strat == 'jet'):\n",
    "    data_dir = path_prefix + 'jets/training/'\n",
    "    data_filenames = {'pp':data_dir+'piplus.root','p0':data_dir+'pi0.root'}\n",
    "\n",
    "# adjust our model and plot paths, so that they are unique for each strategy\n",
    "paths = [modelpath, plotpath]\n",
    "for i in range(len(paths)):\n",
    "    path = paths[i]\n",
    "    path = path + strat\n",
    "    try: os.makedirs(path)\n",
    "    except: pass\n",
    "    path = path + '/'\n",
    "    paths[i] = path\n",
    "modelpath, plotpath = paths\n",
    "\n",
    "# we get uproot trees and pandas DataFrames,\n",
    "# for training + validation\n",
    "tree_name = 'ClusterTree'\n",
    "branches = ['truthE', 'clusterE', 'clusterPt', 'clusterEta', 'cluster_ENG_CALIB_TOT']\n",
    "\n",
    "data_trees = {key:ur.open(val)[tree_name] for key,val in data_filenames.items()}\n",
    "data_frames = {key:val.pandas.df(branches,flatten=False) for key,val in data_trees.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing training and validation samples\n",
    "\n",
    "As we're taking logarithms  of `clusterE` and `cluster_ENG_CALIB_TOT`, we will always perform cuts to have `clusterE` > 0 for all datasets, and `cluster_ENG_CALIB_TOT` > 0 for training. \n",
    "\n",
    "On top of those cuts, we're free to apply additional cuts to training, validation and testing data. We can do them below, as we pick event indices for each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_frames = {}\n",
    "validation_frames = {}\n",
    "\n",
    "# First, the minimum energy cut that we will always apply to data. Anything that fails to pass this cut will be discarded,\n",
    "# we will never evaluate on events that don't pass this cut.\n",
    "global_energy_cut = 0.\n",
    "\n",
    "# We apply a lower cut on cluster_ENG_CALIB_TOT, as very low-energy clusters can throw off training.\n",
    "energy_cut = [0., -1.]\n",
    "if(strat == 'pion' or strat == 'pion_reweighted'): \n",
    "    energy_cut[0] = 5.0e-1 # GeV\n",
    "    energy_cut[1] = -1.\n",
    "    \n",
    "elif(strat == 'jet'): \n",
    "    energy_cut[0] = 5.0e-2 # GeV\n",
    "    energy_cut[1] = -1.\n",
    "\n",
    "data_indices = {} # indices of all usable data, i.e. non-zero energy\n",
    "training_indices = {} # indices of events actually used for training\n",
    "validation_indices = {} # indices of events not used for training (but usable)\n",
    "\n",
    "# percent of events to hand over from training to testing\n",
    "testing_frac = 0.2\n",
    "rng = np.random.default_rng() # for shuffling indices when splitting training/testing\n",
    "\n",
    "for key in data_frames.keys():\n",
    "    \n",
    "    n = len(data_frames[key])\n",
    "    eng_calib_tot = data_frames[key]['cluster_ENG_CALIB_TOT'].to_numpy()\n",
    "    selected_indices = eng_calib_tot > energy_cut[0]\n",
    "    if(energy_cut[1] > 0.): selected_indices = selected_indices * (eng_calib_tot < energy_cut[1])\n",
    "    \n",
    "    selected_indices = selected_indices.nonzero()[0] # from boolean array to a list of actual indices\n",
    "    rng.shuffle(selected_indices)\n",
    "    n_test = int(0.2 * len(selected_indices))\n",
    "    \n",
    "    # making boolean arrays to select events -- arrays are of same length as dataframe\n",
    "    validation_indices[key] = np.full(n,False)\n",
    "    training_indices[key] = np.full(n,False)\n",
    "    validation_indices[key][np.sort(selected_indices[:n_test])] = True\n",
    "    training_indices[key][np.sort(selected_indices[n_test:])] = True\n",
    "    training_frames[key] = data_frames[key][training_indices[key]].copy()\n",
    "    validation_frames[key] = data_frames[key][validation_indices[key]].copy()\n",
    "\n",
    "# #     validation_indices[key] = (data_frames[key]['cluster_ENG_CALIB_TOT'] > global_energy_cut).to_numpy()\n",
    "# #     validation_indices[key] = validation_indices[key]^(validation_indices[key] * training_indices[key])\n",
    "\n",
    "    data_indices[key] = (data_frames[key]['cluster_ENG_CALIB_TOT'] > global_energy_cut).to_numpy()\n",
    "    data_frames[key] = data_frames[key][data_indices[key]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we treat $\\pi^+$ and $\\pi^-$ as the same, let's combine them so that we have *charged* and *neutral* pions. We will store all the charged pions under the key `pp`, and delete the key `pm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of charged pion training/testing events: 261873/65468\n",
      "Number of neutral pion training/testing events: 196960/49240\n"
     ]
    }
   ],
   "source": [
    "key_conversion = {'pp':'charged pion','p0':'neutral pion'}\n",
    "# combining dataframes\n",
    "if('pm' in data_frames.keys()):\n",
    "    data_frames['pp'] = data_frames['pp'].append(data_frames['pm'])\n",
    "    del data_frames['pm']\n",
    "    \n",
    "    training_frames['pp'] = training_frames['pp'].append(training_frames['pm'])\n",
    "    del training_frames['pm']\n",
    "    \n",
    "    validation_frames['pp'] = validation_frames['pp'].append(validation_frames['pm'])\n",
    "    del validation_frames['pm']\n",
    "\n",
    "for key in data_frames.keys():\n",
    "    print('Number of {type} training/testing events: {val1}/{val2}'.format(type=key_conversion[key], val1 = np.sum(training_indices[key]), val2 = np.sum(validation_indices[key])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have many more charged pions than neutral pions, so this *may* result in our charged pion regression being better-trained (unless the stats for both are sufficiently high)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some regression vars\n",
    "for key,frame in data_frames.items():\n",
    "    frame['logE'] = np.log(frame['clusterE'])\n",
    "    frame['logECalib'] = np.log(frame['cluster_ENG_CALIB_TOT'])\n",
    "    \n",
    "for key,frame in training_frames.items():\n",
    "    frame['logE'] = np.log(frame['clusterE'])\n",
    "    frame['logECalib'] = np.log(frame['cluster_ENG_CALIB_TOT'])\n",
    "\n",
    "for key,frame in validation_frames.items():\n",
    "    frame['logE'] = np.log(frame['clusterE'])\n",
    "    frame['logECalib'] = np.log(frame['cluster_ENG_CALIB_TOT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# create scalers\n",
    "scaler_e = {key:StandardScaler() for key in data_frames.keys()}\n",
    "scaler_cal = {key:StandardScaler() for key in data_frames.keys()}\n",
    "scaler_eta = {key:StandardScaler() for key in data_frames.keys()}\n",
    "\n",
    "# fit our scalers, using the training data\n",
    "for key, frame in training_frames.items():\n",
    "    scaler_e[key].fit(frame['logE'].to_numpy().reshape(-1,1))\n",
    "    scaler_cal[key].fit(frame['logECalib'].to_numpy().reshape(-1,1))\n",
    "    scaler_eta[key].fit(frame['clusterEta'].to_numpy().reshape(-1,1))\n",
    "    \n",
    "# now apply our scalers to our data (training, testing and combo)\n",
    "for key, frame in training_frames.items():\n",
    "    frame['s_logE'] = scaler_e[key].transform(frame['logE'].to_numpy().reshape(-1,1))\n",
    "    frame['s_logECalib'] = scaler_cal[key].transform(frame['logECalib'].to_numpy().reshape(-1,1))\n",
    "    frame['s_eta'] = scaler_eta[key].transform(frame['clusterEta'].to_numpy().reshape(-1,1))\n",
    "    \n",
    "for key, frame in validation_frames.items():\n",
    "    frame['s_logE'] = scaler_e[key].transform(frame['logE'].to_numpy().reshape(-1,1))\n",
    "    frame['s_logECalib'] = scaler_cal[key].transform(frame['logECalib'].to_numpy().reshape(-1,1))\n",
    "    frame['s_eta'] = scaler_eta[key].transform(frame['clusterEta'].to_numpy().reshape(-1,1))\n",
    "\n",
    "for key, frame in data_frames.items():\n",
    "    frame['s_logE'] = scaler_e[key].transform(frame['logE'].to_numpy().reshape(-1,1))\n",
    "    frame['s_logECalib'] = scaler_cal[key].transform(frame['logECalib'].to_numpy().reshape(-1,1))\n",
    "    frame['s_eta'] = scaler_eta[key].transform(frame['clusterEta'].to_numpy().reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib as jl\n",
    "# now we save our scalers to a file, so that we can use them when evaluating the model elsewhere\n",
    "scalers = {'e':scaler_e, 'cal':scaler_cal, 'eta':scaler_eta}\n",
    "scaler_file = modelpath + 'scalers.save'\n",
    "if(saveModel and not loadModel): jl.dump(scalers,scaler_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To finish training data preparation, we concatenate the calorimeter images, and then combine them with columns `s_logE` and `s_Eta`.\n",
    "\n",
    "To avoid running out of memory -- which can be an issue depending on where this notebook is run -- we can perform some garbage collection and delete the contents of `calo_images` as they are incorporated into `All_input`, our combined network training input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a handy function for taking a DataFrame (with our scaler-derived columns) and our tree with calo images, and getting the actual network input we need. The `indices` argument is just used for `dtree`, whereas `dframe` should already have the indices applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CombinedInput(dframe, dtree, indices=-1, merge=True):\n",
    "    # Prepare the calo images for input to training.\n",
    "    l = len(layers) * len(dtree.keys())\n",
    "    i = 0\n",
    "    pfx = 'Loading calo images:      '\n",
    "    sfx = 'Complete'\n",
    "    bl = 50\n",
    "    qu.printProgressBarColor(i, l, prefix=pfx, suffix=sfx, length=bl)\n",
    "\n",
    "    calo_images = {}\n",
    "    for key in dtree.keys():\n",
    "        calo_images[key] = {}\n",
    "    \n",
    "        for layer in layers:\n",
    "            if(indices != -1): calo_images[key][layer] = mu.setupCells(dtree[key],layer, indices = indices[key])\n",
    "            else: calo_images[key][layer] = mu.setupCells(dtree[key],layer)\n",
    "            i += 1\n",
    "            qu.printProgressBarColor(i, l, prefix=pfx, suffix=sfx, length=bl)\n",
    "    \n",
    "    #TODO: A bit hacky, but works for now. Dealing with DataFrames and uproot TTree's together is awful...\n",
    "    # Importantly, the merge here is in the same order as above.\n",
    "    if(merge):\n",
    "        for layer in layers:\n",
    "            calo_images['pp'][layer] = np.row_stack((calo_images['pp'][layer],calo_images['pm'][layer]))\n",
    "        del calo_images['pm']\n",
    "        \n",
    "    # Concatenate images, and prepare our combined input.\n",
    "    All_input = {}\n",
    "    keys = list(calo_images.keys())\n",
    "    l = 3 * len(keys)\n",
    "    i = 0\n",
    "    pfx = 'Preparing combined input: '\n",
    "    qu.printProgressBarColor(i, l, prefix=pfx, suffix=sfx, length=bl)\n",
    "\n",
    "    for key in keys:\n",
    "        combined_images = np.concatenate(tuple([calo_images[key][layer] for layer in layers]), axis=1)\n",
    "        del calo_images[key] # delete this element of calo_images, it has been copied and is no longer needed\n",
    "        i = i + 1\n",
    "        qu.printProgressBarColor(i, l, prefix=pfx, suffix=sfx, length=bl)\n",
    "\n",
    "        s_combined,scaler_combined = mu.standardCells(combined_images, layers)\n",
    "        i = i + 1\n",
    "        qu.printProgressBarColor(i, l, prefix=pfx, suffix=sfx, length=bl)\n",
    "\n",
    "        All_input[key] = np.column_stack((dframe[key]['s_logE'], dframe[key]['s_eta'],s_combined))\n",
    "        i = i + 1\n",
    "        qu.printProgressBarColor(i, l, prefix=pfx, suffix=sfx, length=bl)\n",
    "    return All_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading calo images:       |\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m| 100.0% Complete\n",
      "Preparing combined input:  |\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m\u001b[32m█\u001b[0m| 100.0% Complete\n"
     ]
    }
   ],
   "source": [
    "All_input = CombinedInput(training_frames, data_trees,training_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may optionally perform some re-weighting of our training events. If using the `pion_reweighted` strategy, we will re-weight our single-pion training data to match the topo-cluster $p_T$ spectrum of our jet data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_weights = {}\n",
    "if(strat == 'pion_reweighted'):\n",
    "    pt_min = 0.\n",
    "    pt_max = 20.\n",
    "    pt_bins = 100\n",
    "    \n",
    "    # Get the jet files for reweighting. We look in jets/ to make sure we only use the relevant files.\n",
    "    jet_files = glob.glob(path_prefix + 'jets/data/pion/*.root')\n",
    "    \n",
    "    for key in training_frames.keys():\n",
    "        h_train    = rt.TH1F(qu.RN(), 'h_train',   pt_bins, pt_min, pt_max)\n",
    "        h_reweight = rt.TH1F(qu.RN(), 'h_reweight',pt_bins, pt_min, pt_max)\n",
    "\n",
    "        # fill training distribution\n",
    "        training_vals = training_frames[key]['clusterPt'].to_numpy()\n",
    "        for entry in training_vals: h_train.Fill(entry)\n",
    "        h_train.Scale(1./h_train.Integral())\n",
    "        \n",
    "        # fill the reweighting distribution\n",
    "        for file in jet_files:\n",
    "            for entry in ur.open(file)['ClusterTree'].array('clusterPt').flatten():\n",
    "                h_reweight.Fill(entry)\n",
    "        h_reweight.Scale(1./h_reweight.Integral())\n",
    "        h_reweight = h_reweight / h_train\n",
    "        \n",
    "        # now get a list of weights for our events\n",
    "        sample_weights[key] = np.array([h_reweight.GetBinContent(h_reweight.FindBin(x)) for x in training_vals])\n",
    "    \n",
    "else:  sample_weights = {key:np.full(len(All_input[key]), 1.) for key in All_input.keys()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training \"all\" model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the third cell of this section, we define the model -- here you can modify hyper-parameters, or switch to an entirely different model. I will consider changing this to make it a little more streamlined in the future, for easily trying a bunch of models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "Number of devices: 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # disable some of the tensorflow info printouts, only display errors\n",
    "import tensorflow as tf\n",
    "strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\"])\n",
    "ngpu = strategy.num_replicas_in_sync\n",
    "print ('Number of devices: {}'.format(ngpu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 938)               880782    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 938)               880782    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 469)               440391    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 470       \n",
      "=================================================================\n",
      "Total params: 2,202,425\n",
      "Trainable params: 2,202,425\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from models import baseline_nn_All_model\n",
    "\n",
    "lr = 1e-4\n",
    "decay = 1e-6\n",
    "dropout = -1. # < 0 -> no dropout\n",
    "model = baseline_nn_All_model(strategy, lr=lr, decay=decay, dropout=dropout)\n",
    "\n",
    "print(model().summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our regressors (actual networks) that we will fit (train).\n",
    "batch_size = 200 * ngpu\n",
    "epochs = 100 # 100\n",
    "verbose = 1\n",
    "regressors = {key: KerasRegressor(build_fn=model, batch_size=batch_size, epochs=epochs, verbose=verbose) for key in All_input.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the models and save them, or load them from files.\n",
    "model_file_names = {'pp':modelpath + 'all_charged.h5','p0':modelpath + 'all_neutral.h5'}\n",
    "assert (set(model_file_names.keys()) == set(All_input.keys()))\n",
    "histories = {}\n",
    "\n",
    "for key, filename in model_file_names.items():\n",
    "    if not loadModel: histories[key] = regressors[key].fit(All_input[key], training_frames[key]['s_logECalib'], sample_weight = sample_weights[key])\n",
    "    else: regressors[key].model = load_model(model_file_names[key])\n",
    "\n",
    "    if saveModel: regressors[key].model.save(model_file_names[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now evaluate the networks on *all* our data to see how they've done. This will involve overwriting some training data variables that we won't need for plotting results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, the training data\n",
    "print('Getting predictions for training data.')\n",
    "for key, frame in training_frames.items():\n",
    "    frame['clusterE_pred'] = np.exp(scaler_cal[key].inverse_transform(regressors[key].predict(All_input[key])))\n",
    "\n",
    "# Now regenerate All_input for our testing data, and get predictions for it\n",
    "print('Getting predictions for testing data.')\n",
    "All_input = CombinedInput(validation_frames, data_trees, validation_indices)\n",
    "for key, frame in validation_frames.items():\n",
    "    frame['clusterE_pred'] = np.exp(scaler_cal[key].inverse_transform(regressors[key].predict(All_input[key])))\n",
    "\n",
    "# Now regenerate All_input for training + testing, get predictions.\n",
    "All_input = CombinedInput(data_frames, data_trees, data_indices)\n",
    "for key, frame in data_frames.items():\n",
    "    frame['clusterE_pred'] = np.exp(scaler_cal[key].inverse_transform(regressors[key].predict(All_input[key])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting results (testing how well our network works)\n",
    "\n",
    "Now, let's plot some kinematics and network results. We'll make two groups of plots -- one for charged pions and one for neutral pions.\n",
    "\n",
    "Within each group of plots, we'll make two plots for each quantity -- one made using just the training data, and then one made using all the data (training + whatever we excluded -- but still excluding events with `cluster_ENG_CALIB_TOT` $< 0$ since these blow up network output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Median(values, counts, default = 1):\n",
    "    nvar = len(values)\n",
    "    assert(len(counts) == nvar)\n",
    "    if(nvar == 0): return default\n",
    "    value_list = [[values[i] for j in range(int(counts[i]))] for i in range(nvar)]\n",
    "    value_array = [item for sublist in value_list for item in sublist]\n",
    "    value_array = np.array(value_array)\n",
    "    if(len(value_array) == 0): return default\n",
    "    med = np.median(value_array)\n",
    "    return med"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SetColor(hist, color, alpha = 1., style=0):\n",
    "    hist.SetFillColorAlpha(color, alpha)\n",
    "    hist.SetLineColor(color)\n",
    "    if(style != 0): hist.SetFillStyle(style)\n",
    "    return\n",
    "\n",
    "# a simple version of Max's cool 2D plots\n",
    "def EnergyPlot2D(e1, e2, title='title;x;y', nbins = [100,35], x_range = [0.,2000.], y_range = [0.3, 1.7], offset=False, mode = 'median'):\n",
    "    hist = rt.TH2F(qu.RN(), title, nbins[0], x_range[0], x_range[1], nbins[1], y_range[0], y_range[1])\n",
    "    x_vals = e2\n",
    "    y_vals = e1/e2\n",
    "    if(offset): x_vals = x_vals + 1.\n",
    "    \n",
    "    for i in range(len(e2)):\n",
    "        hist.Fill(x_vals[i],y_vals[i])\n",
    "        \n",
    "    # now we want to make a curve representing the medians/means in y\n",
    "    bin_centers_y = np.array([hist.GetYaxis().GetBinCenter(i+1) for i in range(nbins[1])])\n",
    "    mean_vals = np.zeros(nbins[0])\n",
    "    for i in range(nbins[0]):\n",
    "        weights = np.array([hist.GetBinContent(i+1, j+1) for j in range(nbins[1])])\n",
    "        if(mode == 'median'):\n",
    "            mean_vals[i] = Median(bin_centers_y,np.array(weights,dtype=np.dtype('i8')))\n",
    "        else:\n",
    "            if(np.sum(weights) != 0.):\n",
    "                weights = weights / np.sum(weights)\n",
    "                y_vals = np.array([hist.GetYaxis().GetBinCenter(j+1) for j in range(nbins[1])])\n",
    "                y_vals = np.multiply(y_vals,weights)\n",
    "                mean_vals[i] = np.sum(y_vals)\n",
    "            else: mean_vals[i] = 1.\n",
    "        \n",
    "    x_vals = np.array([hist.GetXaxis().GetBinCenter(i+1) for i in range(nbins[0])])\n",
    "    curve = rt.TGraph(nbins[0],x_vals, mean_vals)\n",
    "    curve.SetLineColor(rt.kRed)\n",
    "    curve.SetLineWidth(2)\n",
    "    return curve, hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some ranges for the plots\n",
    "if(strat == 'pion' or strat == 'pion_reweighted'):\n",
    "    max_energy = 2000. # GeV\n",
    "    max_energy_2d = max_energy\n",
    "    bin_energy = 300\n",
    "    ratio_range_2d = [0.3, 1.7]\n",
    "    bins_2d = [200,70]\n",
    "    offset_2d = False\n",
    "    \n",
    "else:\n",
    "    max_energy = 100\n",
    "    max_energy_2d = 10.\n",
    "    bin_energy = 20\n",
    "    ratio_range_2d = [0., 5.]\n",
    "    bins_2d = [50,125]\n",
    "    offset_2d = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterE = {}\n",
    "clusterE_calib = {}\n",
    "clusterE_pred = {}\n",
    "clusterE_true = {}\n",
    "clusterE_ratio1 = {} # ratio of predicted cluster E to calibrated cluster E\n",
    "clusterE_ratio2 = {} # ratio of reco cluster E to calibrated cluster E\n",
    "clusterE_ratio2D = {} # 2D plot, ratio1 versus calibrated cluster E\n",
    "clusterE_ratio2D_zoomed = {} # 2D plot, ratio1 versus calibrated cluster E, zoomed\n",
    "\n",
    "mean_curves = {}\n",
    "mean_curves_zoomed = {}\n",
    "\n",
    "canvs = {}\n",
    "legends = {}\n",
    "stacks = {}\n",
    "\n",
    "key_conversions = {\n",
    "    'pp':'#pi^{#pm}',\n",
    "    'p0':'#pi^{0}',\n",
    "    'check':'#pi^{+} (check)'}\n",
    "\n",
    "dsets = {\n",
    "    'train': training_frames,\n",
    "    'valid': validation_frames,\n",
    "    'all data': data_frames\n",
    "}\n",
    "\n",
    "extensions = ['pdf','png']\n",
    "plot_size = 750\n",
    "\n",
    "for key in ['p0','pp']:\n",
    "    clusterE[key] = {}\n",
    "    clusterE_calib[key] = {}\n",
    "    clusterE_pred[key] = {}\n",
    "    clusterE_true[key] = {}\n",
    "    clusterE_ratio1[key] = {}\n",
    "    clusterE_ratio2[key] = {}\n",
    "    clusterE_ratio2D[key] = {}\n",
    "    clusterE_ratio2D_zoomed[key] = {}\n",
    "    \n",
    "    mean_curves[key] = {}\n",
    "    mean_curves_zoomed[key] = {}\n",
    "\n",
    "    canvs[key] = {}\n",
    "    legends[key] = {}\n",
    "    stacks[key] = {}\n",
    "    \n",
    "    for dkey, frame in dsets.items():\n",
    "        key2 = '(' + key_conversions[key] + ', ' + dkey + ')'\n",
    "        clusterE[key][dkey] = rt.TH1F(qu.RN(), 'E_{reco} ' + key2 +'; E_{reco} [GeV];Count' , bin_energy,0.,max_energy)\n",
    "        clusterE_calib[key][dkey] = rt.TH1F(qu.RN(), 'E_{calib}^{tot} ' + key2 + ';E_{calib}^{tot} [GeV];Count', bin_energy,0.,max_energy)\n",
    "        clusterE_pred[key][dkey] = rt.TH1F(qu.RN(), 'E_{pred} ' + key2 + ';E_{pred} [GeV];Count', bin_energy,0.,max_energy)\n",
    "        clusterE_true[key][dkey] = rt.TH1F(qu.RN(), 'E_{true} ' + key2 + ';E_{true} [GeV];Count', bin_energy,0.,max_energy)\n",
    "        clusterE_ratio1[key][dkey] = rt.TH1F(qu.RN(), 'E_{pred} / E_{calib}^{tot} ' + key2 + ';E_{pred}/E_{calib}^{tot};Count', 250,0.,10.)\n",
    "        clusterE_ratio2[key][dkey] = rt.TH1F(qu.RN(), 'E / E_{calib}^{tot} ' + key2 + ';E_{reco}/E_{calib}^{tot]};Count', 250,0.,10.)\n",
    "\n",
    "        SetColor(clusterE[key][dkey], rt.kViolet, alpha = 0.4)\n",
    "        SetColor(clusterE_calib[key][dkey], rt.kPink + 9, alpha = 0.4)\n",
    "        SetColor(clusterE_pred[key][dkey], rt.kGreen, alpha = 0.4)\n",
    "        SetColor(clusterE_true[key][dkey], rt.kRed, alpha = 0.4)\n",
    "        SetColor(clusterE_ratio1[key][dkey], rt.kViolet, alpha = 0.4)\n",
    "        SetColor(clusterE_ratio2[key][dkey], rt.kGreen, alpha = 0.4)\n",
    "\n",
    "        meas   = frame[key]['clusterE'].to_numpy()\n",
    "        calib  = frame[key]['cluster_ENG_CALIB_TOT'].to_numpy()\n",
    "        pred   = frame[key]['clusterE_pred'].to_numpy()\n",
    "        true   = frame[key]['truthE'].to_numpy()\n",
    "        ratio1 = pred / calib\n",
    "        ratio2 = meas / calib\n",
    "    \n",
    "        for i in range(len(meas)):\n",
    "            clusterE[key][dkey].Fill(meas[i])\n",
    "            clusterE_calib[key][dkey].Fill(calib[i])\n",
    "            clusterE_pred[key][dkey].Fill(pred[i])\n",
    "            clusterE_true[key][dkey].Fill(true[i])\n",
    "            clusterE_ratio1[key][dkey].Fill(ratio1[i])\n",
    "            clusterE_ratio2[key][dkey].Fill(ratio2[i])\n",
    "            \n",
    "        # fill the histogram stacks\n",
    "        stacks[key][dkey] = rt.THStack(qu.RN(), clusterE_ratio1[key][dkey].GetTitle())\n",
    "        stacks[key][dkey].Add(clusterE_ratio1[key][dkey])\n",
    "        stacks[key][dkey].Add(clusterE_ratio2[key][dkey])\n",
    "\n",
    "        # 2D plots\n",
    "        title = 'E_{pred}/E_{calib}^{tot} vs. E_{calib}^{tot} ' + key2 + ';E_{calib}^{tot} [GeV];E_{pred}/E_{calib}^{tot};Count'\n",
    "        x_range = [0.,max_energy_2d]\n",
    "        nbins = bins_2d\n",
    "        mean_curves[key][dkey], clusterE_ratio2D[key][dkey] = EnergyPlot2D(pred, calib, nbins = nbins, x_range = x_range, y_range = ratio_range_2d, title=title, offset=True)\n",
    "\n",
    "        title = 'E_{pred}/E_{calib}^{tot} vs. E_{calib}^{tot} ' + key2 + ';(E_{calib}^{tot} + 1) [GeV];E_{pred}/E_{calib}^{tot};Count'\n",
    "        x_range = [1.,1. + 0.01 * max_energy_2d]\n",
    "        nbins = bins_2d\n",
    "        nbins[0] = nbins[0] - 1\n",
    "        mean_curves_zoomed[key][dkey], clusterE_ratio2D_zoomed[key][dkey] = EnergyPlot2D(pred, calib, nbins = nbins, x_range = x_range, y_range = ratio_range_2d, title=title, offset=False)\n",
    "\n",
    "    plots = [clusterE, clusterE_calib, clusterE_pred, clusterE_true, stacks, clusterE_ratio2D, clusterE_ratio2D_zoomed]\n",
    "    dkeys = list(dsets.keys())\n",
    "    # make legend for the overlapping plots\n",
    "    legends[key] = rt.TLegend(0.7,0.7,0.85,0.85)\n",
    "    legends[key].SetBorderSize(0)\n",
    "    legends[key].AddEntry(clusterE_ratio1[key][dkeys[0]],'x = pred','f')\n",
    "    legends[key].AddEntry(clusterE_ratio2[key][dkeys[0]],'x = reco','f')\n",
    "    \n",
    "    nx = len(dkeys)\n",
    "    ny = len(plots)\n",
    "    canvs[key] = rt.TCanvas(qu.RN(),'c_'+str(key),nx * plot_size,ny * plot_size)\n",
    "    canvs[key].Divide(nx,ny)\n",
    "    \n",
    "    for i, plot in enumerate(plots):\n",
    "        \n",
    "        if(plot == stacks):\n",
    "            x = nx * i + 1\n",
    "            for j, dkey in enumerate(dkeys):\n",
    "                canvs[key].cd(x + j)\n",
    "                plot[key][dkey].Draw('NOSTACK HIST')\n",
    "                rt.gPad.SetLogy()\n",
    "                rt.gPad.SetGrid()\n",
    "                plot[key][dkey].GetHistogram().GetXaxis().SetTitle('E_{x}/E_{calib}^{tot}')\n",
    "                plot[key][dkey].GetHistogram().GetYaxis().SetTitle(clusterE_ratio1[key][dkey].GetYaxis().GetTitle())\n",
    "                \n",
    "                if(strat == 'jet'):\n",
    "                    plot[key][dkey].SetMinimum(5.0e-1)\n",
    "                    plot[key][dkey].SetMaximum(1.0e3)\n",
    "                    \n",
    "                else:\n",
    "                    plot[key][dkey].SetMinimum(5.0e-1)\n",
    "                    plot[key][dkey].SetMaximum(2.0e5)   \n",
    "                    \n",
    "                legends[key].SetTextColor(dark_style.text)\n",
    "                legends[key].Draw()\n",
    "\n",
    "        elif(plot == clusterE_ratio2D or plot == clusterE_ratio2D_zoomed):\n",
    "            x = nx * i + 1\n",
    "            for j, dkey in enumerate(dkeys):\n",
    "                canvs[key].cd(x + j)\n",
    "                plot[key][dkey].Draw('COLZ')\n",
    "                if(plot == clusterE_ratio2D):\n",
    "                    mean_curves[key][dkey].Draw('SAME')\n",
    "\n",
    "                else: \n",
    "                    mean_curves_zoomed[key][dkey].Draw('SAME')\n",
    "                    rt.gPad.SetLogx()\n",
    "                    rt.gPad.SetBottomMargin(0.15)\n",
    "                    plot[key][dkey].GetXaxis().SetTitleOffset(1.5)\n",
    "                    \n",
    "                rt.gPad.SetLogz()\n",
    "                rt.gPad.SetRightMargin(0.2)\n",
    "                plot[key][dkey].GetXaxis().SetMaxDigits(4)\n",
    "                  \n",
    "        else:\n",
    "            x = nx * i + 1\n",
    "            for j, dkey in enumerate(dkeys):\n",
    "                canvs[key].cd(x + j)\n",
    "                plot[key][dkey].Draw('HIST')\n",
    "                rt.gPad.SetLogy()\n",
    "    \n",
    "    # Draw the canvas\n",
    "    canvs[key].Draw()\n",
    "    \n",
    "    # Save the canvas as a PDF & PNG image.\n",
    "    image_name = key + '_plots'\n",
    "    for ext in extensions: canvs[key].SaveAs(plotpath + image_name + '.' + ext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For `pion`:\n",
    "\n",
    "    - Both of our regressions appear to be working quite well -- we see that our predicted energy (bottom left plot, dark blue) outperforms the reconstructed energy (bottom left plot, cyan) as the peaks aren't as wide while still being centered on $1$ as we would like.\n",
    "    \n",
    "For `jet`:\n",
    "\n",
    "    - Note that our energy range is quite different than with our `pion` training data. It seems that the isolated pions were often far more energetic -- or at least led to far more energetic topo-clusters -- than do our jets. The results don't look terrible when evaluating within the training set, but we see that including events that weren't used for training, which corresponds with very low `cluster_ENG_CALIB_TOT` values, throws things off. The network extrapolates very poorly towards these lower energies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Just double-checking the range of cluster_ENG_CALIB_TOT in the training/plotting data\n",
    "# minimum = np.min(data_frames['pp']['cluster_ENG_CALIB_TOT'].to_numpy())\n",
    "# maximum = np.max(data_frames['pp']['cluster_ENG_CALIB_TOT'].to_numpy())\n",
    "# print('({val1:.1e},{val2:.1e})'.format(val1=minimum, val2=maximum))"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
